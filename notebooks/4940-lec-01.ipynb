{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6114136c",
   "metadata": {},
   "source": [
    "# Lecture 01: Inference with Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730d5cb9",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59b5d15",
   "metadata": {},
   "source": [
    "Let's start with a classic language model, OpenAI's GPT-2. \n",
    "\n",
    "Hugging Face has a simple way to generate text with a model like this: you create a text generation `pipeline`, pass it some text, and you get a completion out the other end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "007b6596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "356ae09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5655eac52f2f4276a64276cb24f4ab68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel LOAD REPORT from: gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec6de1d",
   "metadata": {},
   "source": [
    "We can ignore this warning above. Let's generate some text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f262d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=50) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Welcome to INFO 4940 at Cornell! I'm your instructor, and I'm always here to help you learn!\n",
       "\n",
       "Check us out at our Facebook Page!!!\n",
       "\n",
       "Facebook Page @ Cornell University\n",
       "\n",
       "Facebook Page @ Cornell University\n",
       "\n",
       "Facebook Page @ Cornell University"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generated_sequence = generator(\n",
    "          \"Welcome to INFO 4940 at Cornell! I'm your instructor,\",\n",
    "          max_new_tokens=50, \n",
    "          num_return_sequences=1\n",
    "          )\n",
    "\n",
    "display(Markdown(generated_sequence[0][\"generated_text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e54348a",
   "metadata": {},
   "source": [
    "Two things to notice here:\n",
    "- GPT-2 is not a chatbot: it is just *completing* the text we pass it. If you recall the first day's lecture, it's just continually \"filling in the blank\" starting from the text we give it. \n",
    "- The generated text can be pretty weird. Why do you think that is? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2384f5c0",
   "metadata": {},
   "source": [
    "## Loading models and tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376170a8",
   "metadata": {},
   "source": [
    "For now, let's imagine that most of our model is a black box: we don't know how it works on the inside, but we can give it something and observe what it gives us in return. The `pipeline` approach simplifies inference tremendously, but it's ultimately not very useful if we want to see how our model generates predictions based on our inputs. \n",
    "\n",
    "Let's go a tiny bit deeper, while still hand waving away most of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "903c0af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ce5e58",
   "metadata": {},
   "source": [
    "Language models don't actually ingest strings, but rather *tokens*. These tokens are indices in the model's *vocabulary*. In other words, we need a way to convert our input (a string) into tokens (a corresponding list of integers).\n",
    "\n",
    "Every text generation model on Hugging Face has its own tokenizer which handles this process for you. We will talk about how tokenizers actually work later, but for now, you should familiarize yourself with a process that looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "092ef0dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a689002d20b477fb1fb533e7f5e354d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel LOAD REPORT from: gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"gpt2\"\n",
    "\n",
    "# Download and load our model from Hugging Face\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "\n",
    "# Download and load its tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d876a9",
   "metadata": {},
   "source": [
    "Unlike the `pipeline` approach earlier, we now can inspect our model directly. Let's do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4a38ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dcfe38",
   "metadata": {},
   "source": [
    "This is probably all pretty meaningless at this point, and that is totally fine. What's important here is that the model is loaded on our device and we can inspect, modify, add, or remove any part of it. \n",
    "\n",
    "Remember how big, ChatGPT-scale models can have trillions of parameters? Let's see how many parameters GPT-2 has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4700a36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters for gpt2: 124,439,808\n"
     ]
    }
   ],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters for {model_name_or_path}: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5e62ef",
   "metadata": {},
   "source": [
    "Compared to something like ChatGPT, GPT-2 is **really** small. In fact, it's about one fifth the size of the smallest class of LLMs nowadays (0.6B).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93da4851",
   "metadata": {},
   "source": [
    "Let's return to our tokenizer. What does it do? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a121884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [15496, 995, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42e844e",
   "metadata": {},
   "source": [
    "Like we said above, the tokenizer maps a string to a sequence of integers. These integers correspond to tokens in the model's vocabulary. We can inspect the vocabulary like so: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ca298c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ġhorizontal': 16021,\n",
       " 'ĠSega': 29490,\n",
       " 'ottesville': 23806,\n",
       " 'wing': 5469,\n",
       " 'orses': 11836,\n",
       " 'ĠAssist': 43627,\n",
       " 'ĠJagu': 21117,\n",
       " 'ĠOpera': 26049,\n",
       " 'pent': 16923,\n",
       " 'ĠSing': 5573,\n",
       " 'Ġbranches': 13737,\n",
       " 'innacle': 37087,\n",
       " 'Ġnause': 24480,\n",
       " 'Flash': 30670,\n",
       " 'ĠViol': 13085,\n",
       " 'Ġperceptions': 23574,\n",
       " 'iot': 5151,\n",
       " 'Ġaccol': 45667,\n",
       " 'ĠSamoa': 43663,\n",
       " 'internal': 32538,\n",
       " 'Ġbroadband': 18729,\n",
       " 'andre': 49078,\n",
       " 'ocre': 27945,\n",
       " 'ĠNay': 38808,\n",
       " 'Ġ8': 807,\n",
       " 'OUT': 12425,\n",
       " 'utra': 35076,\n",
       " 'xxx': 31811,\n",
       " 'ĠRichie': 49209,\n",
       " 'erk': 9587,\n",
       " 'Ġspeak': 2740,\n",
       " 'xus': 40832,\n",
       " 'Ġlogically': 34193,\n",
       " '.</': 25970,\n",
       " 'Ġgreatly': 9257,\n",
       " '94': 5824,\n",
       " 'Ġlaun': 2698,\n",
       " 'ĠMagnus': 35451,\n",
       " 'ĠFrames': 36291,\n",
       " 'Ġnavigate': 16500,\n",
       " 'Ġsupper': 43743,\n",
       " 'Ġpromised': 8072,\n",
       " 'ĠProfit': 42886,\n",
       " 'ĠIPM': 50200,\n",
       " 'ĠKiller': 19900,\n",
       " 'mc': 23209,\n",
       " 'rent': 1156,\n",
       " 'Ġimprisonment': 16510,\n",
       " 'ĠGreenland': 30155,\n",
       " 'ĠPresent': 21662,\n",
       " 'Ġpivotal': 28992,\n",
       " 'Ġbuilders': 31606,\n",
       " 'Ġbillions': 13188,\n",
       " 'Ġlumber': 34840,\n",
       " 'ĠKarachi': 46154,\n",
       " 'gement': 16025,\n",
       " '_': 62,\n",
       " 'Ġworkshop': 20243,\n",
       " 'ĠVR': 6453,\n",
       " 'Ġbeasts': 21930,\n",
       " 'wn': 675,\n",
       " 'ĠDrag': 12697,\n",
       " 'llah': 22734,\n",
       " 'Ġultimate': 8713,\n",
       " 'ĠProvided': 29750,\n",
       " 'Ġ1886': 49539,\n",
       " 'Offic': 12710,\n",
       " 'ea': 18213,\n",
       " 'Ġwicked': 20589,\n",
       " 'Ġlicence': 17098,\n",
       " 'al': 282,\n",
       " 'ĠGeo': 32960,\n",
       " 'Ġtotality': 44861,\n",
       " 'Ġlabels': 14722,\n",
       " 'Ġtriple': 15055,\n",
       " 'Nob': 21191,\n",
       " 'administ': 39081,\n",
       " 'ĠUseful': 49511,\n",
       " 'hetamine': 25385,\n",
       " 'Ġaffluent': 34552,\n",
       " 'Ġ................': 44713,\n",
       " 'fetched': 50012,\n",
       " 'global': 20541,\n",
       " 'ĠYug': 25554,\n",
       " 'Ġ1968': 15963,\n",
       " 'Ġenraged': 37530,\n",
       " 'Ġtre': 2054,\n",
       " 'tp': 34788,\n",
       " 'Ġintercept': 15788,\n",
       " 'ĠRh': 10323,\n",
       " 'Ġcomeback': 21933,\n",
       " 'Ġnationals': 22970,\n",
       " 'bull': 16308,\n",
       " 'Ġoutlandish': 47284,\n",
       " 'leen': 20042,\n",
       " 'Analy': 37702,\n",
       " 'Ġintroducing': 16118,\n",
       " 'ĠRAF': 28168,\n",
       " 'Sanders': 26747,\n",
       " 'Ġbeat': 4405,\n",
       " 'ĠPrincess': 8449,\n",
       " 'Ġlegendary': 13273,\n",
       " 'omez': 30010,\n",
       " '],[': 38430,\n",
       " 'Ġtroublesome': 35778,\n",
       " 'icc': 44240,\n",
       " 'Ġstarved': 47224,\n",
       " 'SHIP': 49423,\n",
       " 'Ġ470': 38634,\n",
       " 'Ġstealth': 19159,\n",
       " 'ĠGMOs': 46135,\n",
       " 'outside': 43435,\n",
       " 'Ġbargaining': 23189,\n",
       " 'ãħĭãħĭ': 40345,\n",
       " 'Ġsil': 3313,\n",
       " 'ĠGab': 12300,\n",
       " 'ĠMongolia': 44364,\n",
       " 'Ġunintentional': 48398,\n",
       " 'deg': 13500,\n",
       " 'ĠGian': 30851,\n",
       " 'ĠDill': 44322,\n",
       " 'Ġprized': 37768,\n",
       " 'etooth': 16271,\n",
       " 'ĠHitchcock': 49817,\n",
       " 'tun': 28286,\n",
       " 'Ġ179': 27228,\n",
       " 'Ġfashion': 6977,\n",
       " 'ttes': 13036,\n",
       " 'During': 7191,\n",
       " 'Ġtheir': 511,\n",
       " 'Rest': 19452,\n",
       " 'Ġcompliance': 11846,\n",
       " 'Ġfuneral': 14825,\n",
       " 'Ġnostalgic': 40459,\n",
       " 'aling': 4272,\n",
       " 'element': 30854,\n",
       " 'Alan': 36235,\n",
       " 'Ġcrude': 14897,\n",
       " 'Ġdoor': 3420,\n",
       " 'ĠHear': 26319,\n",
       " 'Ġprofile': 7034,\n",
       " 'Ġlifetime': 10869,\n",
       " 'aterasu': 45335,\n",
       " 'ortal': 16906,\n",
       " 'justice': 31012,\n",
       " 'Ġantitrust': 42766,\n",
       " 'ĠLOVE': 27889,\n",
       " 'ĠAux': 47105,\n",
       " 'Ġticket': 7846,\n",
       " 'verning': 13974,\n",
       " 'Ġâľĵ': 24762,\n",
       " 'ĠJord': 38317,\n",
       " 'Ġclarification': 31321,\n",
       " 'ĠIntrodu': 11036,\n",
       " 'App': 4677,\n",
       " 'Ġfabulous': 28294,\n",
       " 'Ġsmells': 25760,\n",
       " 'Ġhijab': 48492,\n",
       " 'Ġquotation': 35777,\n",
       " 'ĠSell': 25688,\n",
       " 'oleon': 25637,\n",
       " 'Ġsubmarines': 34031,\n",
       " 'Ġlyr': 47188,\n",
       " '167': 21940,\n",
       " 'glas': 14391,\n",
       " 'Else': 40674,\n",
       " 'ĠZig': 24992,\n",
       " 'ĠEgg': 14562,\n",
       " 'Ġphotos': 5205,\n",
       " 'anish': 7115,\n",
       " 'Ġguy': 3516,\n",
       " 'Left': 18819,\n",
       " 'urate': 15537,\n",
       " 'ĠJust': 2329,\n",
       " 'Ġbullets': 16043,\n",
       " 'Ġtox': 8293,\n",
       " 'Ġobtain': 7330,\n",
       " 'Civil': 32610,\n",
       " 'Ġcafes': 46221,\n",
       " 'idently': 46046,\n",
       " 'eria': 5142,\n",
       " 'Ġeyeing': 48847,\n",
       " 'byte': 26327,\n",
       " 'Ġnice': 3621,\n",
       " 'Ġfuck': 5089,\n",
       " 'antam': 49653,\n",
       " 'container': 34924,\n",
       " 'Feature': 38816,\n",
       " 'ĠRousse': 42849,\n",
       " 'Ġsmokes': 44589,\n",
       " 'aditional': 27013,\n",
       " 'ĠðŁĳ': 50169,\n",
       " 'ampunk': 46183,\n",
       " 'Ġeliminate': 11005,\n",
       " 'Ġtwisting': 36373,\n",
       " 'Ã©r': 42445,\n",
       " 'Ġfunny': 8258,\n",
       " 'Ġwarned': 7728,\n",
       " 'Dur': 36927,\n",
       " 'ĠSports': 7092,\n",
       " '782': 46519,\n",
       " 'Ġmurd': 7847,\n",
       " 'Sky': 22308,\n",
       " 'Ġtuned': 16524,\n",
       " 'Ġfoss': 10967,\n",
       " 'ĠCommunist': 14884,\n",
       " 'makers': 6620,\n",
       " 'astrous': 20168,\n",
       " 'Ġimagin': 25007,\n",
       " 'Counter': 31694,\n",
       " 'ociation': 41003,\n",
       " 'Ġmasked': 29229,\n",
       " 'Ġ1800': 21431,\n",
       " 'Ġgenius': 15632,\n",
       " 'itt': 715,\n",
       " 'Graphics': 18172,\n",
       " 'Ġpastor': 22175,\n",
       " 'ĠFacility': 29118,\n",
       " 'ĠBlock': 9726,\n",
       " '.$': 48082,\n",
       " 'Ġacoustic': 26071,\n",
       " 'esian': 35610,\n",
       " 'olk': 13597,\n",
       " 'Format': 26227,\n",
       " '###': 21017,\n",
       " 'Ġincorporating': 29927,\n",
       " 'Ġallowable': 49299,\n",
       " 'Ġsalvage': 32504,\n",
       " 'Ġhierarch': 28398,\n",
       " 'ĠBoo': 21458,\n",
       " 'ĠMeow': 42114,\n",
       " 'Ġearlier': 2961,\n",
       " 'Ġpi': 31028,\n",
       " 'Ġtextures': 20028,\n",
       " 'Ġtend': 4327,\n",
       " 'ĠGord': 39592,\n",
       " 'ALSE': 23719,\n",
       " 'dding': 33403,\n",
       " 'ĠPrairie': 41196,\n",
       " 'Ġequations': 27490,\n",
       " 'ta': 8326,\n",
       " 'Ġanyways': 32845,\n",
       " 'head': 2256,\n",
       " 'Ġembro': 27623,\n",
       " 'ĠDollars': 45792,\n",
       " 'ĠChern': 37243,\n",
       " 'active': 5275,\n",
       " 'Ġexpanding': 11581,\n",
       " 'ĠEdison': 37448,\n",
       " 'ĠHonduras': 32982,\n",
       " 'Ġarr': 5240,\n",
       " 'Ġnano': 38706,\n",
       " 'ĠYoung': 6960,\n",
       " 'imi': 25236,\n",
       " 'Ġcoy': 34003,\n",
       " 'Ġpromotes': 21068,\n",
       " 'Ġrecruits': 23096,\n",
       " 'Although': 7003,\n",
       " 'kh': 14636,\n",
       " 'Ġworshipped': 46204,\n",
       " 'morph': 24503,\n",
       " 'Ġsalesman': 42414,\n",
       " 'Paris': 40313,\n",
       " 'Ġhiatus': 37009,\n",
       " 'ĠDeVos': 39503,\n",
       " 'ĠOng': 48041,\n",
       " 'ĠStre': 9737,\n",
       " 'Ġsuperhero': 17343,\n",
       " 'Attempt': 37177,\n",
       " 'ills': 2171,\n",
       " 'Ġdecriminal': 38126,\n",
       " 'Ġobliged': 23278,\n",
       " 'esar': 18964,\n",
       " 'ĠNOTICE': 28536,\n",
       " 'Ġseventeen': 38741,\n",
       " 'Ess': 29508,\n",
       " 'Ġ299': 31011,\n",
       " 'Ġreign': 13580,\n",
       " 'antic': 5109,\n",
       " 'ĠKEY': 35374,\n",
       " 'cul': 3129,\n",
       " 'ĠTenn': 9034,\n",
       " 'ĠJinn': 48653,\n",
       " 'paying': 32629,\n",
       " 'Ġairspace': 34216,\n",
       " 'roying': 38295,\n",
       " 'ĠRW': 33212,\n",
       " 'Ġunderscore': 44810,\n",
       " 'CVE': 31436,\n",
       " 'ropolis': 25986,\n",
       " 'ination': 1883,\n",
       " 'ãĤ³': 24679,\n",
       " 'ocl': 38679,\n",
       " 'ÃĥÃĤÃĥÃĤ': 5815,\n",
       " 'ĠVigil': 39840,\n",
       " 'product': 11167,\n",
       " 'EngineDebug': 49781,\n",
       " 'Ġ387': 49689,\n",
       " 'Chief': 23675,\n",
       " 'Ġ12': 1105,\n",
       " 'arch': 998,\n",
       " 'Ġnever': 1239,\n",
       " 'illac': 40607,\n",
       " '986': 49087,\n",
       " 'InstoreAndOnline': 40241,\n",
       " 'Ball': 23410,\n",
       " 'Ġ701': 48173,\n",
       " 'Ġadoptive': 49378,\n",
       " 'Ġfrail': 41890,\n",
       " 'Ġdick': 19317,\n",
       " 'aze': 6201,\n",
       " 'ppelin': 48425,\n",
       " 'Si': 42801,\n",
       " 'Damage': 22022,\n",
       " 'ĠBoy': 6387,\n",
       " 'ĠShare': 8734,\n",
       " 'Ġvoices': 10839,\n",
       " '476': 35435,\n",
       " 'Ġintern': 1788,\n",
       " 'ĠPhi': 47256,\n",
       " 'ĠDK': 32975,\n",
       " 'OLD': 15173,\n",
       " 'ĠMorton': 35766,\n",
       " 'ERA': 46461,\n",
       " 'Ġgentle': 10296,\n",
       " 'LM': 31288,\n",
       " 'Ġmutually': 26519,\n",
       " 'Ġlaws': 3657,\n",
       " 'ĠEditorial': 39525,\n",
       " 'oxide': 28885,\n",
       " 'ĠKaty': 36715,\n",
       " 'ĠHockey': 17277,\n",
       " 'ĠDrawn': 50061,\n",
       " 'Ġdelet': 28128,\n",
       " 'ĠVIDE': 45748,\n",
       " 'Ġrotate': 23064,\n",
       " 'Ġcropped': 48998,\n",
       " 'Ġconstellation': 38712,\n",
       " 'Ġdivisions': 17397,\n",
       " 'Ġundermines': 37001,\n",
       " 'ĠCour': 2734,\n",
       " 'ĠConn': 20776,\n",
       " 'Ġdonations': 10976,\n",
       " 'ĠGTA': 30807,\n",
       " 'Ġsecret': 3200,\n",
       " 'Ġknots': 33620,\n",
       " 'ĠMushroom': 36482,\n",
       " 'ĠCol': 1623,\n",
       " 'Ġreducing': 8868,\n",
       " 'Ġdominion': 43866,\n",
       " 'ĠCompanion': 30653,\n",
       " 'Ġsnipers': 50238,\n",
       " 'ileen': 42236,\n",
       " 'Ġtipped': 28395,\n",
       " 'Ġcoaching': 13101,\n",
       " 'Ġ1863': 47072,\n",
       " 'Ġinstallation': 9988,\n",
       " 'ĠBetween': 14307,\n",
       " 'Ġrethink': 36437,\n",
       " '490': 31503,\n",
       " 'ĠRequ': 9394,\n",
       " 'Ġenthusiast': 33471,\n",
       " '20439': 47936,\n",
       " 'Ġprogramming': 8300,\n",
       " 'Ġdetached': 30795,\n",
       " 'Ġsped': 40424,\n",
       " 'Ġabsurdity': 41793,\n",
       " 'Ġtuna': 38883,\n",
       " 'Ġexercise': 5517,\n",
       " 'ENN': 34571,\n",
       " 'Ġmere': 5019,\n",
       " 'ĠSphere': 31798,\n",
       " 'dash': 42460,\n",
       " 'Ġdisliked': 43252,\n",
       " 'Choose': 31851,\n",
       " 'Ġindoor': 22639,\n",
       " 'ĠClassics': 40184,\n",
       " 'imura': 43817,\n",
       " 'Teen': 45639,\n",
       " 'stone': 6440,\n",
       " 'Ġreaches': 12229,\n",
       " 'ĠPlants': 39269,\n",
       " 'Ġsafe': 3338,\n",
       " 'wheel': 22001,\n",
       " 'Ġspots': 10222,\n",
       " 'Alternatively': 44163,\n",
       " 'ĠCre': 5844,\n",
       " 'ĠINTO': 39319,\n",
       " 'feed': 12363,\n",
       " 'ĠPsyNet': 36130,\n",
       " 'ĠNETWORK': 49791,\n",
       " 'undown': 41609,\n",
       " 'iversal': 11480,\n",
       " 'Ġtracks': 8339,\n",
       " 'Ġunnecessarily': 38347,\n",
       " 'Ġalterations': 29858,\n",
       " 'Ġpower': 1176,\n",
       " 'occupied': 28756,\n",
       " 'Ġresidency': 27308,\n",
       " 'ĠOR': 6375,\n",
       " 'repair': 49932,\n",
       " 'css': 25471,\n",
       " 'Ġ610': 44300,\n",
       " 'Ġcultivating': 45414,\n",
       " 'Ġshootings': 17690,\n",
       " '321': 36453,\n",
       " 'alore': 40612,\n",
       " 'ibility': 2247,\n",
       " 'Ġdens': 29509,\n",
       " 'Ġbrow': 4772,\n",
       " 'Ġaffirm': 16266,\n",
       " 'Ġhospitalized': 31736,\n",
       " 'Ġexplores': 25409,\n",
       " 'Ġfreedom': 4925,\n",
       " 'Dou': 40287,\n",
       " 'ĠMalcolm': 20002,\n",
       " 'Quick': 21063,\n",
       " 'Ġouter': 12076,\n",
       " 'ĠBind': 41211,\n",
       " 'Attribute': 33682,\n",
       " 'ĠChief': 5953,\n",
       " 'hai': 44488,\n",
       " 'Be': 3856,\n",
       " 'Ġcolourful': 44799,\n",
       " 'Ġtru': 45768,\n",
       " 'UK': 15039,\n",
       " 'inion': 23971,\n",
       " 'Within': 22005,\n",
       " '178': 23188,\n",
       " 'Ġ1934': 29300,\n",
       " 'peed': 39492,\n",
       " 'Ġhypotheses': 35125,\n",
       " 'ĠDuncan': 18625,\n",
       " 'ÙĲ': 44208,\n",
       " 'black': 13424,\n",
       " 'ĠIRA': 27859,\n",
       " '\"!': 40484,\n",
       " 'Ġexquisite': 40123,\n",
       " 'Ġregeneration': 27597,\n",
       " 'circ': 21170,\n",
       " 'ĠCold': 10250,\n",
       " 'Ġimm': 2296,\n",
       " 'ĠVolunte': 28935,\n",
       " 'Ġusher': 36642,\n",
       " 'Ġdepicted': 18904,\n",
       " 'Ġgamers': 15072,\n",
       " 'Ġrev': 2710,\n",
       " 'Ġcompuls': 27954,\n",
       " 'ession': 2521,\n",
       " 'ĠAberdeen': 44985,\n",
       " 'Ġknowingly': 26526,\n",
       " 'Ġmuscle': 8280,\n",
       " 'Ġdepiction': 33016,\n",
       " 'ede': 18654,\n",
       " 'âĺħ': 15583,\n",
       " 'ĠUpdated': 19433,\n",
       " 'Ġpurposely': 39033,\n",
       " '*****': 35625,\n",
       " 'becue': 31927,\n",
       " 'Ġtaught': 7817,\n",
       " 'igo': 14031,\n",
       " 'Linux': 19314,\n",
       " 'Ġmandate': 14598,\n",
       " 'ones': 1952,\n",
       " 'ß': 155,\n",
       " 'Ġdocker': 36253,\n",
       " 'Ġthe': 262,\n",
       " 'OU': 2606,\n",
       " 'ĠInstant': 24470,\n",
       " 'ĠObs': 11086,\n",
       " '!),': 26290,\n",
       " 'ĠLyme': 46401,\n",
       " 'ĠNewcastle': 22410,\n",
       " 'ribute': 4163,\n",
       " 'ĠDarth': 31152,\n",
       " 'Ġbully': 27410,\n",
       " 'ĠInstallation': 32588,\n",
       " 'Ġcurry': 34611,\n",
       " 'ĠWE': 12887,\n",
       " 'Ġinhabited': 30671,\n",
       " 'Ġseeded': 48453,\n",
       " 'Ġcontent': 2695,\n",
       " 'Ġvolley': 32967,\n",
       " 'ĠSir': 7361,\n",
       " 'Ġchloride': 44921,\n",
       " 'ĠFW': 48849,\n",
       " 'ĠKindle': 27114,\n",
       " '392': 32321,\n",
       " 'hatt': 11653,\n",
       " 'Ġsuing': 28941,\n",
       " 'ĠLarry': 13633,\n",
       " 'ĠZika': 30861,\n",
       " 'ivable': 21911,\n",
       " 'Ġcommunication': 6946,\n",
       " 'Jason': 26497,\n",
       " 'riors': 8657,\n",
       " 'Ġenabled': 9343,\n",
       " 'Ġexplo': 6694,\n",
       " 'Uber': 39018,\n",
       " 'Ġlegisl': 3560,\n",
       " 'ĠJuven': 48113,\n",
       " 'Fall': 24750,\n",
       " 'ooters': 48316,\n",
       " 'struct': 7249,\n",
       " 'ĠEp': 4551,\n",
       " 'Ġ150': 6640,\n",
       " 'ĠCSV': 44189,\n",
       " 'Task': 25714,\n",
       " 'ãĥĥ': 14777,\n",
       " '590': 36993,\n",
       " 'ĠTablet': 42399,\n",
       " 'oad': 1170,\n",
       " 'ĠKendrick': 38643,\n",
       " 'ĠBridges': 41839,\n",
       " 'Ġbat': 7365,\n",
       " 'ĠUltimate': 11165,\n",
       " 'ĠOrtiz': 39039,\n",
       " 'Target': 21745,\n",
       " 'Ġextremes': 31082,\n",
       " 'ĠiP': 9736,\n",
       " 'oly': 3366,\n",
       " 'ĠJo': 5302,\n",
       " 'ĠSevent': 35734,\n",
       " 'Ġsatellite': 11210,\n",
       " 'ĠReplay': 23635,\n",
       " 'Ġgeek': 27314,\n",
       " 'Ġkindergarten': 38717,\n",
       " 'Ġwretched': 41857,\n",
       " 'Ġresponsive': 21802,\n",
       " 'ounge': 20891,\n",
       " 'Ġtheolog': 37275,\n",
       " 'THE': 10970,\n",
       " 'ĠSexy': 49131,\n",
       " 'ĠU': 471,\n",
       " 'ĠAdvertisement': 39711,\n",
       " 'ĠHume': 45485,\n",
       " 'Ġfeaturing': 9593,\n",
       " 'edo': 24757,\n",
       " 'Ġconsensual': 39685,\n",
       " 'cery': 12757,\n",
       " 'Alias': 40489,\n",
       " 'antry': 21238,\n",
       " 'Ġdubbed': 17494,\n",
       " 'ĠLatest': 26603,\n",
       " 'Ġshootout': 34995,\n",
       " 'kHz': 44191,\n",
       " 'ĠScorpion': 48821,\n",
       " 'Ġbye': 33847,\n",
       " 'ent': 298,\n",
       " 'Indust': 35848,\n",
       " 'Ġcorrelation': 16096,\n",
       " 'ilic': 41896,\n",
       " 'Ġvag': 14334,\n",
       " 'ĠInclude': 40348,\n",
       " 'Ġstrain': 14022,\n",
       " '442': 39506,\n",
       " 'Registered': 47473,\n",
       " 'ĠSubtle': 47419,\n",
       " 'Ġprotective': 14153,\n",
       " 'Ġstationary': 31607,\n",
       " 'ĠNewspaper': 49598,\n",
       " 'Ġshared': 4888,\n",
       " 'bish': 31795,\n",
       " 'Ġunmanned': 35272,\n",
       " 'working': 16090,\n",
       " 'Ġhol': 6039,\n",
       " 'EU': 19684,\n",
       " 'ĠâĹ': 24966,\n",
       " 'Ġrestrain': 39300,\n",
       " 'ĠMethodist': 38029,\n",
       " 'ĠTent': 30765,\n",
       " 'Ġunnecess': 11689,\n",
       " 'âĸĵ': 38626,\n",
       " 'ĠBian': 41227,\n",
       " 'ĠCoch': 33005,\n",
       " 'Ġoutraged': 27697,\n",
       " 'Ġnearest': 16936,\n",
       " 'Ġolds': 44979,\n",
       " 'Ġpayable': 28538,\n",
       " 'âĶĢâĶĢ': 8418,\n",
       " 'Ġstark': 19278,\n",
       " 'Ġnm': 28642,\n",
       " 'pard': 26037,\n",
       " 'Ġdialog': 17310,\n",
       " 'fork': 32523,\n",
       " 'ĠJonas': 40458,\n",
       " 'Ġveterans': 11255,\n",
       " 'who': 8727,\n",
       " 'Ġslaughtered': 33850,\n",
       " 'ĠSCP': 17527,\n",
       " 'ĠDeb': 8965,\n",
       " '>>>>': 16471,\n",
       " 'adas': 38768,\n",
       " 'Middle': 34621,\n",
       " 'Ġlevers': 48872,\n",
       " 'Ġprotestors': 36915,\n",
       " 'Ġposture': 24521,\n",
       " 'ĠViz': 36339,\n",
       " 'xx': 5324,\n",
       " 'Ġvaccinations': 46419,\n",
       " 'Ġconsultations': 42052,\n",
       " 'ĠLaurel': 43442,\n",
       " 'ilege': 41866,\n",
       " 'ĠBoris': 25026,\n",
       " 'Lewis': 40330,\n",
       " 'INTON': 46812,\n",
       " 'Ġcontracting': 29148,\n",
       " 'ĠNieto': 49783,\n",
       " 'Ġpessim': 32277,\n",
       " 'Alpha': 38077,\n",
       " 'ĠWi': 11759,\n",
       " 'hemoth': 34394,\n",
       " '[': 58,\n",
       " 'Ġnaked': 12105,\n",
       " 'Ġurban': 7876,\n",
       " 'Ġinfuri': 35905,\n",
       " 'ibel': 43837,\n",
       " 'Tele': 31709,\n",
       " 'Ġsettle': 12259,\n",
       " 'ĠKelvin': 46577,\n",
       " 'Ġpolitely': 34313,\n",
       " 'ĠKatie': 24721,\n",
       " 'éĹĺ': 42234,\n",
       " 'ĠWeber': 28137,\n",
       " 'chie': 3043,\n",
       " 'ĠConsumers': 45103,\n",
       " 'Ġdisbanded': 47302,\n",
       " 'ĠUFOs': 49863,\n",
       " 'ĠLarson': 42630,\n",
       " 'argon': 37920,\n",
       " 'Ġsolder': 42809,\n",
       " 'Ġepid': 12851,\n",
       " 'Ġcountered': 32925,\n",
       " 'Ġcell': 2685,\n",
       " 'riction': 46214,\n",
       " 'Ġpowd': 32279,\n",
       " 'order': 2875,\n",
       " 'arij': 39010,\n",
       " 'isites': 31327,\n",
       " 'ĠUrs': 37935,\n",
       " 'ĠBlessed': 33398,\n",
       " 'faces': 32186,\n",
       " 'DEN': 41819,\n",
       " 'Ġgoof': 31644,\n",
       " 'ĠLeap': 33927,\n",
       " 'anda': 5282,\n",
       " 'Ġstew': 20798,\n",
       " 'ĠModel': 9104,\n",
       " '470': 27790,\n",
       " 'reme': 2182,\n",
       " 'Ġunfortunately': 12716,\n",
       " 'anim': 11227,\n",
       " 'Ġemployers': 11390,\n",
       " 'tab': 8658,\n",
       " 'Ġtyrann': 44315,\n",
       " '820': 41739,\n",
       " 'Ġduplication': 50124,\n",
       " 'Ġstaffed': 47331,\n",
       " 'Ġsits': 10718,\n",
       " 'Ġshutting': 25136,\n",
       " 'Ġtickets': 8587,\n",
       " 'amph': 28474,\n",
       " 'Ġloved': 6151,\n",
       " 'Ġmonkey': 21657,\n",
       " 'iffin': 42022,\n",
       " 'armed': 12026,\n",
       " 'Ġblack': 2042,\n",
       " 'CHO': 44899,\n",
       " 'Ġelastic': 27468,\n",
       " 'ĠONLY': 22224,\n",
       " 'ĠCaf': 35046,\n",
       " 'Ġthresholds': 40885,\n",
       " 'Ġepoch': 36835,\n",
       " 'Ġrodents': 41093,\n",
       " '523': 49803,\n",
       " 'Ġtanker': 46335,\n",
       " 'ĠHAVE': 21515,\n",
       " 'Ġeight': 3624,\n",
       " 'Ġpatio': 39660,\n",
       " 'Ġpalms': 39513,\n",
       " 'rip': 5528,\n",
       " 'hor': 17899,\n",
       " 'ossal': 33582,\n",
       " 'Ġsegregated': 38135,\n",
       " 'ĠDance': 17349,\n",
       " 'Ġarbitrary': 14977,\n",
       " 'Ġdiscipline': 12883,\n",
       " 'Ġlicensee': 35796,\n",
       " ']+': 48688,\n",
       " 'Ġcocktails': 34281,\n",
       " 'ĠCustom': 8562,\n",
       " 'Ġobstacles': 17648,\n",
       " 'à¥': 24231,\n",
       " 'ĠTotal': 7472,\n",
       " 'Ġinfluenced': 12824,\n",
       " 'Ġcherry': 23612,\n",
       " 'Ġorally': 47311,\n",
       " 'Ġraids': 20371,\n",
       " 'anian': 38336,\n",
       " 'BE': 12473,\n",
       " 'Ġorchestrated': 37728,\n",
       " 'Ġdiscreet': 38724,\n",
       " 'ipp': 3974,\n",
       " 'ĠPlain': 28847,\n",
       " 'ĠSud': 14818,\n",
       " 'Ġkilling': 5170,\n",
       " 'Ġbould': 47069,\n",
       " 'ietal': 21587,\n",
       " 'IL': 4146,\n",
       " 'tained': 4644,\n",
       " 'Hig': 36124,\n",
       " 'ĠBend': 26874,\n",
       " 'ĠTreasury': 13419,\n",
       " 'Provider': 29495,\n",
       " 'Ġpointer': 17562,\n",
       " 'nir': 32986,\n",
       " 'bus': 10885,\n",
       " 'ĠUnited': 1578,\n",
       " 'Ġlackluster': 50146,\n",
       " 'ĠTravel': 13524,\n",
       " 'alysed': 47557,\n",
       " 'Ġnude': 26349,\n",
       " 'Ġuncon': 21254,\n",
       " 'olor': 45621,\n",
       " 'ĠMercenary': 49969,\n",
       " 'heavy': 23701,\n",
       " 'Ġveggies': 39425,\n",
       " 'ĠSpot': 15899,\n",
       " 'ĠmM': 47676,\n",
       " 'ĠShel': 15325,\n",
       " 'Ġtruthful': 42278,\n",
       " 'Ġreconstruct': 31081,\n",
       " 'çī': 31965,\n",
       " 'ideshow': 42286,\n",
       " 'activity': 21797,\n",
       " 'ĠPaper': 14962,\n",
       " 'web': 12384,\n",
       " 'Ġboolean': 25131,\n",
       " 'otos': 14163,\n",
       " 'ĠStreets': 27262,\n",
       " \"'/\": 26488,\n",
       " 'Ġironic': 25304,\n",
       " 'Ġclosing': 9605,\n",
       " 'Imm': 24675,\n",
       " 'addle': 37382,\n",
       " 'Aug': 12512,\n",
       " 'TR': 5446,\n",
       " 'Ġcontained': 7763,\n",
       " 'aste': 4594,\n",
       " 'Ġlug': 49419,\n",
       " 'Ġfibers': 26742,\n",
       " 'ĠLesbian': 43797,\n",
       " 'ĠUnt': 26970,\n",
       " 'anded': 12249,\n",
       " 'Ġuber': 48110,\n",
       " 'Ġpraying': 26002,\n",
       " 'Ġissu': 25731,\n",
       " 'ĠNBA': 7403,\n",
       " 'Ġunconditional': 42423,\n",
       " 'creation': 38793,\n",
       " 'rift': 35357,\n",
       " 'category': 22872,\n",
       " 'ĠRadeon': 13082,\n",
       " 'Ġstumbled': 24241,\n",
       " 'Ġsong': 3496,\n",
       " 'Ġ900': 15897,\n",
       " 'abilia': 48249,\n",
       " 'Ġexerted': 48322,\n",
       " 'Ġproceeding': 18788,\n",
       " 'Ġwest': 7421,\n",
       " 'Ġprevail': 28615,\n",
       " 'Ġarticulated': 36877,\n",
       " 'Ġperennial': 39983,\n",
       " 'lem': 10671,\n",
       " 'ĠRib': 23133,\n",
       " 'Ġsecuring': 19732,\n",
       " 'stars': 30783,\n",
       " 'ĠCharity': 37575,\n",
       " 'Ġmigrant': 23088,\n",
       " 'Ġsmoothly': 21461,\n",
       " 'Ġsup': 7418,\n",
       " 'Ġfingerprint': 25338,\n",
       " 'Ġgirl': 2576,\n",
       " 'Ġunle': 15809,\n",
       " 'Ġpowers': 5635,\n",
       " 'Ġlinguistic': 29929,\n",
       " 'ĠAnon': 49347,\n",
       " 'HD': 10227,\n",
       " 'ocalypse': 15145,\n",
       " 'ĠKatz': 36290,\n",
       " 'Ġretire': 8058,\n",
       " 'armac': 32813,\n",
       " 'Ġcasualties': 18499,\n",
       " 'Ġinsults': 27819,\n",
       " 'Ġpreempt': 38726,\n",
       " 'bag': 21454,\n",
       " 'ĠSeoul': 22372,\n",
       " 'ĠLAT': 42355,\n",
       " 'ĠGeoffrey': 42803,\n",
       " 'ess': 408,\n",
       " 'growing': 25167,\n",
       " 'rolet': 33087,\n",
       " 'Ġdefe': 7503,\n",
       " 'Ġnovelist': 37986,\n",
       " 'ĠMinutes': 23757,\n",
       " 'ĠWHAT': 25003,\n",
       " 'ises': 2696,\n",
       " 'Variable': 43015,\n",
       " 'My': 3666,\n",
       " 'ĠTalks': 47595,\n",
       " 'ĠSchmidt': 24740,\n",
       " 'Ġuncomfortable': 12916,\n",
       " 'Ġdelve': 39130,\n",
       " 'Ġappendix': 43600,\n",
       " 'OIL': 49713,\n",
       " 'Ġepilepsy': 35325,\n",
       " '59': 3270,\n",
       " 'Ġconvergence': 40826,\n",
       " 'Ġreplaced': 6928,\n",
       " 'ĠFaster': 38996,\n",
       " 'ĠCharleston': 26070,\n",
       " 'LY': 11319,\n",
       " 'Ġawaited': 39576,\n",
       " 'Ġswitches': 18225,\n",
       " 'ĠNear': 20173,\n",
       " 'Sac': 38318,\n",
       " 'ussie': 43480,\n",
       " 'ĠPeak': 23974,\n",
       " 'ĠPiet': 36548,\n",
       " 'ographics': 24188,\n",
       " 'Ġheadquartered': 48583,\n",
       " 'animate': 45685,\n",
       " 'Ġdepress': 41447,\n",
       " 'ĠCha': 20703,\n",
       " 'ĠSubjects': 43815,\n",
       " 'ĠVincent': 18653,\n",
       " 'Ġwaged': 34225,\n",
       " 'igm': 17225,\n",
       " 'Ġrefrain': 25133,\n",
       " 'verb': 19011,\n",
       " 'zos': 37925,\n",
       " 'Abyss': 49073,\n",
       " 'Ġautomobiles': 44820,\n",
       " 'ĠStudio': 11733,\n",
       " 'ceptor': 49492,\n",
       " 'Ġsupermarkets': 40875,\n",
       " 'zai': 35142,\n",
       " 'Ġcolony': 18815,\n",
       " 'ands': 1746,\n",
       " 'ĠOptim': 30011,\n",
       " 'Ġste': 2876,\n",
       " 'ĠTwain': 49824,\n",
       " 'Ġpatrolling': 44518,\n",
       " 'client': 16366,\n",
       " 'Push': 49222,\n",
       " 'Volume': 31715,\n",
       " 'added': 29373,\n",
       " 'ĠTower': 8765,\n",
       " 'ĠRevised': 31492,\n",
       " 'Ġourselves': 6731,\n",
       " 'ĠMormon': 13164,\n",
       " '********': 4557,\n",
       " 'elected': 28604,\n",
       " 'Ġmalnutrition': 49290,\n",
       " 'Ġflexible': 12846,\n",
       " 'ĠGemini': 35495,\n",
       " 'ĠHirosh': 35763,\n",
       " 'ĠVed': 36101,\n",
       " '284': 30336,\n",
       " 'ĠVista': 25160,\n",
       " 'ifice': 9680,\n",
       " 'ĠMichel': 12386,\n",
       " 'BILITY': 25382,\n",
       " 'mix': 19816,\n",
       " 'exclusive': 41195,\n",
       " 'ĠDie': 6733,\n",
       " 'ochemistry': 37074,\n",
       " 'Ġbias': 10690,\n",
       " 'Ġovers': 10753,\n",
       " '¨': 101,\n",
       " ').': 737,\n",
       " 'Ġeternity': 28989,\n",
       " 'gas': 22649,\n",
       " 'Ġinspires': 38934,\n",
       " 'ĠSuddenly': 24975,\n",
       " 'Ġfry': 32959,\n",
       " 'ĠJong': 17960,\n",
       " 'ĠPirates': 20169,\n",
       " 'ĠKar': 9375,\n",
       " 'oval': 8325,\n",
       " 'ĠHz': 26109,\n",
       " 'prov': 15234,\n",
       " 'itta': 48519,\n",
       " 'ISE': 24352,\n",
       " 'phas': 5902,\n",
       " 'Ġpatriotic': 33453,\n",
       " 'Prep': 37534,\n",
       " 'ĠSuns': 29139,\n",
       " 'count': 9127,\n",
       " 'ĠManz': 37734,\n",
       " 'Ġjustification': 17734,\n",
       " 'Ġ117': 19048,\n",
       " 'ĠOu': 42201,\n",
       " 'Ġ165': 21409,\n",
       " 'ĠCODE': 42714,\n",
       " 'eric': 35626,\n",
       " 'Ġmasterpiece': 30669,\n",
       " 'Ġhalted': 27771,\n",
       " 'Ġtrenches': 40068,\n",
       " 'ĠSolar': 12347,\n",
       " 'Relations': 47117,\n",
       " 'Ġabund': 12467,\n",
       " 'Ġplay': 711,\n",
       " 'stay': 31712,\n",
       " 'Ġplayed': 2826,\n",
       " 'Ġple': 3339,\n",
       " 'Ġtrack': 2610,\n",
       " 'Ġexacerb': 22907,\n",
       " 'kaya': 35372,\n",
       " 'Ġmaize': 49235,\n",
       " 'Global': 22289,\n",
       " 'ĠPavel': 49612,\n",
       " 'Ġ1942': 22458,\n",
       " 'Ġfootball': 4346,\n",
       " 'ĠISO': 19694,\n",
       " 'Ġspice': 25721,\n",
       " 'ĠHAM': 48079,\n",
       " 'Ġabs': 2352,\n",
       " 'avin': 20637,\n",
       " 'Ġforensic': 23953,\n",
       " 'ĠFerdinand': 44312,\n",
       " 'Philipp': 49680,\n",
       " 'Ġreplied': 8712,\n",
       " 'Ġassass': 10519,\n",
       " 'ĠThousands': 30405,\n",
       " 'ĠLearning': 18252,\n",
       " 'ĠTRE': 43236,\n",
       " 'Ġburn': 4245,\n",
       " 'ull': 724,\n",
       " 'ĠWhether': 10127,\n",
       " 'cloth': 44905,\n",
       " 'Ġdehyd': 28151,\n",
       " 'Ġscanning': 21976,\n",
       " 'Ġdictate': 27861,\n",
       " 'Ġresumed': 28291,\n",
       " 'romeda': 32291,\n",
       " 'Ġfull': 1336,\n",
       " 'Ġassociation': 8112,\n",
       " 'Ġingen': 27016,\n",
       " 'Ġreason': 1738,\n",
       " 'selected': 34213,\n",
       " '2009': 10531,\n",
       " 'ĠEva': 32355,\n",
       " 'Ġdifferentiation': 32488,\n",
       " 'Ġ386': 48340,\n",
       " 'Match': 23850,\n",
       " 'Ġsensing': 34244,\n",
       " 'istics': 3969,\n",
       " 'ctica': 28914,\n",
       " 'ena': 8107,\n",
       " 'ĠAdvance': 25170,\n",
       " 'hemat': 10024,\n",
       " 'ĠGreater': 18169,\n",
       " 'Ġsavior': 47921,\n",
       " 'Ġswoop': 38527,\n",
       " 'Nat': 47849,\n",
       " 'ĠRIGHT': 33621,\n",
       " 'ippi': 12715,\n",
       " 'Ġexpelled': 27307,\n",
       " 'Ġexempt': 13068,\n",
       " 'disable': 40223,\n",
       " 'ĠExplorer': 19142,\n",
       " 'ieft': 49868,\n",
       " 'need': 31227,\n",
       " 'Royal': 41861,\n",
       " 'Ġcaptain': 10654,\n",
       " 'bil': 33473,\n",
       " 'za': 4496,\n",
       " 'Ġgases': 21678,\n",
       " 'isively': 42042,\n",
       " 'Ġnegligible': 36480,\n",
       " 'ascular': 14767,\n",
       " 'Ġ2017': 2177,\n",
       " 'ushed': 7474,\n",
       " 'corruption': 46260,\n",
       " 'ĠTAM': 33112,\n",
       " 'Ġsurrender': 16908,\n",
       " 'Way': 25309,\n",
       " 'prev': 47050,\n",
       " 'Ġdistract': 11786,\n",
       " 'Ġcourage': 11917,\n",
       " 'Ġwelding': 47973,\n",
       " 'Ġobservation': 13432,\n",
       " 'Ġremain': 3520,\n",
       " 'ĠMeteor': 25582,\n",
       " 'ĠSnyder': 22543,\n",
       " 'ĠGNU': 22961,\n",
       " 'powers': 30132,\n",
       " 'Ġseal': 13810,\n",
       " 'Ġlower': 2793,\n",
       " ...}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a74fa572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How big is it? \n",
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211a71ba",
   "metadata": {},
   "source": [
    "You might be wondering about that \"Ġ\" at the beginning of some of the tokens. It's not important now, but that indicates leading whitespace. Models like GPT-2 use a *subword tokenizer* that splits words into multiple tokens. We'll talk more about this later in the course!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12919f14",
   "metadata": {},
   "source": [
    "Here's another example of what these input IDs look like along with the string they correspond to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c6b59a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10778: INFO',\n",
       " '5125: Ġ49',\n",
       " '1821: 40',\n",
       " '25: :',\n",
       " '1374: ĠHow',\n",
       " '27140: ĠLL',\n",
       " '10128: Ms',\n",
       " '5521: ĠWork']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = \"INFO 4940: How LLMs Work\"\n",
    "example_input_ids = tokenizer(example_text)[\"input_ids\"]\n",
    "example_token_strs = tokenizer.convert_ids_to_tokens(example_input_ids)\n",
    "\n",
    "[f\"{id}: {tok}\" for id, tok in zip(example_input_ids, example_token_strs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c6a622",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7369d32",
   "metadata": {},
   "source": [
    "Enough about tokenizers for now--let's use our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a09a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"I visited the Space Needle, Pike Place Market, and Pioneer Square on my trip to\"\n",
    "example_encoding = tokenizer(\n",
    "    example_text,\n",
    "    return_tensors=\"pt\"      # We need tensors, not lists:\n",
    ")                            # our model is actually a PyTorch model\n",
    "                             # under the hood.\n",
    "\n",
    "\n",
    "model.eval()                 # We are not training our model, \n",
    "                             # so we put it in \"eval\" mode.\n",
    "\n",
    "with torch.no_grad():        # Don't need gradients (more on this later)\n",
    "    example_logits = model(**example_encoding)[\"logits\"].squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39c37ab",
   "metadata": {},
   "source": [
    "Ok, what's going on here? \n",
    "\n",
    "1. We encode our text using the `tokenizer`, specifying that we want the output as tensors. This gives us a dictionary of tensors.\n",
    "2. We make sure the `model` is in `eval` mode, and that we aren't calculating gradients. \n",
    "3. We pass our encodings to the model and get logits back out.\n",
    "\n",
    "What are logits in this context? It might help to look at the shape of the tensor: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ea778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_logits.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c106aa",
   "metadata": {},
   "source": [
    "Does that second number look familiar? Here's a hint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc64dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110677bc",
   "metadata": {},
   "source": [
    "What about that first number? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e942f94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_encoding[\"input_ids\"].squeeze().size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12424de0",
   "metadata": {},
   "source": [
    "For every token in our input sequence, we get a vector the size of our vocabulary. In other words, we get a prediction over our vocabulary that is conditioned on the preceding tokens in the sequence. \n",
    "\n",
    "Remember what we said in the first lecture: a language model defines a probability distribution over sequences. That's exactly what the model is doing!\n",
    "\n",
    "However, there's a small catch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf65056",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Logit sum: {example_logits.sum(dim=1)}\")\n",
    "print(f\"Logit max: {example_logits.max(dim=1)[0]}\")\n",
    "print(f\"Logit min: {example_logits.min(dim=1)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18580905",
   "metadata": {},
   "source": [
    "We need a way to convert these raw outputs into a usable probability distribution (every element between 0 and 1, all elements sum to 1). Fortunately, there's a function that does just that: [Softmax](https://docs.pytorch.org/docs/stable/generated/torch.nn.Softmax.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4fabe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_probs = F.softmax(example_logits, dim=1)\n",
    "\n",
    "example_probs.sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aa1d7c",
   "metadata": {},
   "source": [
    "Great!\n",
    "\n",
    "Now, let's recall what our input sequence was: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b4cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce05043",
   "metadata": {},
   "source": [
    "Of course, we could generate more text here, but let's simplify things a little. Let's compare the probabilities computed for a few words given this input sequence. \n",
    "\n",
    "First, we need to get the index of each word's token in the model's vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa9db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [\"Seattle\", \"Portland\", \"Chicago\", \"Houston\"]\n",
    "city_indices = [tokenizer.vocab[\"Ġ\"+c] for c in cities]\n",
    "city_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cb4433",
   "metadata": {},
   "source": [
    "We want to get probabilities given the whole sequence, so we're interested in the last logits. Let's use our vocab indices to get those predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ffde06",
   "metadata": {},
   "outputs": [],
   "source": [
    "for city, prob in zip(cities, example_probs[-1, city_indices]): \n",
    "    print(f\"{city}: {prob:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1725268",
   "metadata": {},
   "source": [
    "As we'd expect, Seattle is the most likely city out of the limited list we presented. However, its absolute probability seems relatively low--let's look at the most likely tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe18ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort probabilities and take the top 10\n",
    "top_predictions = example_probs[-1].argsort(descending=True)[:10]\n",
    "\n",
    "for idx in top_predictions:\n",
    "    token = tokenizer.convert_ids_to_tokens([idx])\n",
    "    prob = example_probs[-1, idx]\n",
    "    print(f\"{token[0].replace('Ġ', '')}: {prob:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae3855f",
   "metadata": {},
   "source": [
    "While this is a toy example, I hope you can see how powerful this is for applications beyond just text generation. I also hope that this demystifies what's going on under the hood for an LLM: even for models several orders of magnitude larger than this, they are ultimately just producing logits from sequences of integers. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info-4940-llms-s26",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
